
## AWS Inferentia

When some inference workloads require  an entire GPU or have low latency requirements, use a specialized and dedicated inference chip. AWS Inferentia chip is a ML inference chip designed to deliver high performance at low cost. Each AWS Inferentia chip provides hundreds of TOPS (tera operations per second).

## Amazon Elastic Inference

Allow you to attach low-cost GPU powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep learning inference by up to 75 %.

Amazon Elastic Inferece accelerators are network-attached devices that work along with EC2 instances in your endpoint to accelerated your inference calls. 